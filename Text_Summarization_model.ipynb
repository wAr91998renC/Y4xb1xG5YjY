{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdK6WKzLY-8a"
      },
      "source": [
        "# Install Essential Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# -----------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XU3_aRitCbl",
        "outputId": "f4ff68b1-1989-4d7a-949b-247c78e84906"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai\n",
            "  Using cached openai-0.26.5.tar.gz (55 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-macosx_11_0_arm64.whl (336 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.9/336.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting tqdm\n",
            "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai) (2.28.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.20->openai) (2022.6.15)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.20->openai) (2.1.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.20->openai) (1.26.11)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.20->openai) (3.3)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp310-cp310-macosx_11_0_arm64.whl (29 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-macosx_11_0_arm64.whl (34 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp310-cp310-macosx_11_0_arm64.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->openai) (22.2.0)\n",
            "Building wheels for collected packages: openai\n",
            "  Building wheel for openai (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for openai: filename=openai-0.26.5-py3-none-any.whl size=67596 sha256=8cf37debbda33b8267719e9fb65d3dcb952d6ca0a3a054855399b7123933465a\n",
            "  Stored in directory: /Users/trongphan/Library/Caches/pip/wheels/17/e0/3d/e7f547caa758526c1a066c1fdfa4995877ef34ea0e7367010e\n",
            "Successfully built openai\n",
            "Installing collected packages: tqdm, multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.26.5 tqdm-4.64.1 yarl-1.8.2\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSfTikpK0Bu5",
        "outputId": "3d191b0c-77f4-496c-d0d8-5439080b2658"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymupdf in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.21.1)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade pymupdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSYtBERigMYj"
      },
      "source": [
        "# -----------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oku1xgSZIu5"
      },
      "source": [
        "# Import API KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# -----------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RfmpkMqT1daE"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "OPENAI_API_KEY = ''\n",
        "with open('/Users/trongphan/Desktop/hoho/OpenAI.json', 'r') as file_to_read:\n",
        "    json_data = json.load(file_to_read)\n",
        "    OPENAI_API_KEY = json_data[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XajkO-wEtnXp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "openai.api_key =  OPENAI_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "srz45bfF0DJ1"
      },
      "outputs": [],
      "source": [
        "import fitz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXHBDyCpgLHE"
      },
      "source": [
        "# -----------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tjh6UssbZXUv"
      },
      "source": [
        "## Transformer Paper Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# -----------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vfWpy7_QVC8t"
      },
      "outputs": [],
      "source": [
        "doc = fitz.open('/Users/trongphan/Downloads/Attention.pdf') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Ni19lQsoVJKP"
      },
      "outputs": [],
      "source": [
        "summary_list =[]\n",
        "for page in doc:\n",
        "  text = page.get_text(\"text\")\n",
        "  #print(text)\n",
        "  prompt= text + \"\\n Tl;dr:\"\n",
        "  response = openai.Completion.create(\n",
        "  model=\"text-davinci-003\",\n",
        "  prompt=prompt,\n",
        "  temperature=0.7,\n",
        "  max_tokens=120,\n",
        "  top_p=0.9,\n",
        "  frequency_penalty=0.0,\n",
        "  presence_penalty=1\n",
        "  )\n",
        "  summary_list.append(response[\"choices\"][0][\"text\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUjynEGNXcbT",
        "outputId": "c5ed1c80-f2ab-4955-d8c2-7830546acfbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " This paper proposes a new network architecture, the Transformer, based solely on attention mechanisms, that is superior to existing models in sequence transduction tasks. It achieves better results in machine translation and constituency parsing while being more parallelizable and requiring less time to train.  The Transformer is a model architecture that eschews recurrence and instead relies entirely on an attention mechanism to draw global dependencies between input and output. It allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. \n",
            "The Transformer model architecture consists of an encoder and decoder stack, each composed of six identical layers. Each layer has two sub-layers; a multi-head self-attention mechanism and a simple, position-wise fully connected feed-forward network. Residual connections are used around each of the sub-layers, followed by layer normalization. The decoder also adds a third sub-layer which performs multi-head attention over the output of the encoder stack. Attention is a function that maps a query and set of key-value pairs to an output, \n",
            "Scaled Dot-Product Attention is an attention mechanism that takes in queries, keys and values of dimension dk, and computes the dot products of the query with all keys, dividing each by √dk and applying a softmax function to obtain the weights on the values. Multi-Head Attention consists of multiple attention layers running in parallel, and is used to project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. The output values are concatenated and once again projected, resulting in the final values.  Multi-head attention allows the model to attend to different representation subspaces at different positions. It consists of multiple attention heads, each with a projection matrix of dmodel/h dimensions and a total computational cost similar to single-head attention. The Transformer uses multi-head attention in three ways: encoder-decoder attention, self-attention in the encoder, and self-attention in the decoder. In addition, it contains position-wise feed-forward networks, embeddings, and positional encoding to provide order information. \n",
            "This paper compares self-attention layers to recurrent and convolutional layers in terms of total computational complexity, amount of parallelizable computation, and maximum path length between long-range dependencies in the network. Self-attention layers are faster than recurrent layers for most tasks, and have a constant number of sequentially executed operations. They also have a shorter maximum path length, making them easier to learn long-range dependencies.  We use a self-attention and point-wise feed-forward network to connect all pairs of input and output positions in our model. We train it on the WMT 2014 English-German and English-French datasets, using the Adam optimizer with residual dropout and label smoothing regularization.  The Transformer model achieved better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost. We used label smoothing during training, beam search with a beam size of 4 and length penalty α = 0.6, and maximum output length during inference of input length + 50. For the base models, we used a single model obtained by averaging the last 5 checkpoints, while for the big models, we averaged the last 20 checkpoints. Results showed that our big model  In Table 3, we see that reducing the attention key size (dk) hurts model quality, suggesting that a more sophisticated compatibility function than dot product may be beneficial. We also see that bigger models are better and that dropout helps to avoid overfitting. In row (E), we replaced our sinusoidal positional encoding with learned positional embeddings and observed nearly identical results to the base model. For English constituency parsing, we found that the Transformer performs well, achieving 92.7 F1 on WSJ Section 23 when using semi-supervised training.  In this paper, we present the Transformer, a sequence transduction model based entirely on attention. We show that it can be trained significantly faster than architectures based on recurrent or convolutional layers and achieves a new state of the art on both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks. We also demonstrate that it can perform surprisingly well even when trained only on small datasets, such as the Wall Street Journal portion of the Penn Treebank.  Recent research on sequence modeling has focused on gated recurrent neural networks, convolutional sequence to sequence learning, and self-attention networks. These models have been used for tasks such as language modeling, machine translation, and image recognition. Other applications include PCFG grammars, long short-term memory networks, active memory systems, Neural GPUs, and Factorization Tricks for LSTM networks. To train these models, algorithms like Adam and self-training are used. Datasets such as the Penn Treebank are used to evaluate the performance of the models.   Recent advances in natural language processing have focused on the use of deep learning models to achieve improved performance in tasks such as machine translation, summarization, parsing, and text understanding. These models include attention-based methods, recurrent neural networks, memory networks, tree-structured models, and mixture-of-experts layers. Additionally, techniques such as dropout, subword units, output embeddings, and sparsely-gated mixtures-of-experts layers are used to improve model performance. \n",
            "Figure 3 shows an example of the attention mechanism which follows long-distance dependencies in the encoder self-attention in layer 5 of 6. The figure shows that many of the attention heads attend to a distant dependency of the verb 'making', completing the phrase 'making...more difficult'. Different colors represent different heads.  Attention heads 5 and 6 in layer 5 of a 6-layer transformer appear to be involved in anaphora resolution, with very sharp attentions for the word 'its'.  Attention heads in deep learning models can learn to focus on different aspects of the input, such as sentence structure, enabling them to perform tasks related to this structure.\n"
          ]
        }
      ],
      "source": [
        "summary_text=' '.join(summary_list)\n",
        "print(summary_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrN3AgYzZHzd",
        "outputId": "cf0c6b8e-3e1e-4403-9f7c-7c2a34f1459e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  This paper presents a new network architecture, the Transformer, which is based solely on attention mechanisms and outperforms existing models in sequence transduction tasks. It is more parallelizable and requires less time to train. The model consists of an encoder-decoder stack of six layers each, with multi-head self-attention and position-wise fully connected feed-forward networks. Scaled Dot-Product Attention is used for computing dot products between queries and keys, followed by a softmax function to obtain weights on values. Results from experiments on WMT 2014 English-to-German and English-to-French translation show improved BLEU scores at a fraction of the training cost compared to previous state-of-the-art models. Label smoothing, beam search, and positional encoding are also used during training and inference.\n"
          ]
        }
      ],
      "source": [
        "prompt= summary_text + \"\\n Tl;dr:\"\n",
        "response = openai.Completion.create(\n",
        "model=\"text-davinci-003\",\n",
        "prompt=prompt,\n",
        "temperature=0.7,\n",
        "max_tokens=400,\n",
        "top_p=0.9,\n",
        "frequency_penalty=0.0,\n",
        "presence_penalty=1\n",
        ")\n",
        "print(response[\"choices\"][0][\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Np0zcc0IajDP",
        "outputId": "a616f7b7-4cd5-4d2a-a84e-66c9286b924f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"stop\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"text\": \"  This paper presents a new network architecture, the Transformer, which is based solely on attention mechanisms and outperforms existing models in sequence transduction tasks. It is more parallelizable and requires less time to train. The model consists of an encoder-decoder stack of six layers each, with multi-head self-attention and position-wise fully connected feed-forward networks. Scaled Dot-Product Attention is used for computing dot products between queries and keys, followed by a softmax function to obtain weights on values. Results from experiments on WMT 2014 English-to-German and English-to-French translation show improved BLEU scores at a fraction of the training cost compared to previous state-of-the-art models. Label smoothing, beam search, and positional encoding are also used during training and inference.\"\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1677315246,\n",
            "  \"id\": \"cmpl-6nkbeHHnMxhuorSjrVr99Zm8yn2aX\",\n",
            "  \"model\": \"text-davinci-003\",\n",
            "  \"object\": \"text_completion\",\n",
            "  \"usage\": {\n",
            "    \"completion_tokens\": 169,\n",
            "    \"prompt_tokens\": 1317,\n",
            "    \"total_tokens\": 1486\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "r4WmT6W5ePIk"
      },
      "outputs": [],
      "source": [
        "summary_list =[]\n",
        "for page in doc:\n",
        "  text = page.get_text(\"text\")\n",
        "  #print(text)\n",
        "  prompt= \"Summarize this for a second-grade student: \" +text \n",
        "  response = openai.Completion.create(\n",
        "  model=\"text-davinci-003\",\n",
        "  prompt=prompt,\n",
        "  temperature=0.7,\n",
        "  max_tokens=120,\n",
        "  top_p=0.9,\n",
        "  frequency_penalty=0.0,\n",
        "  presence_penalty=1\n",
        "  )\n",
        "  summary_list.append(response[\"choices\"][0][\"text\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUYSaUW6f8-H",
        "outputId": "6aa8416a-370e-440f-e116-50230b9a3816"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "This article is about a new type of network called the Transformer which uses attention mechanisms instead of recurrence or convolutions. Experiments on two machine translation tasks have shown that the Transformer is better in quality and requires less time to train than other models. The Transformer achieved 28.4 BLEU on one machine translation task, improving over the existing best results by more than 2 BLEU. It also set a new single-model state-of-the-art score of 41.8 on another task after training for 3.5 days on eight GPUs. This was much A second-grade student can understand that scientists have been working hard to improve machines that can translate from one language to another. They have made machines that can do this faster and better, using something called the \"Transformer\". Summary\n",
            "The Transformer is a model architecture composed of an Encoder and Decoder stack. The Encoder and Decoder have two sub-layers each. They also use Attention, which is a function that uses a query and key-value pairs to create an output vector. \n",
            "Second-grade students can learn about Scaled Dot-Product Attention and Multi-Head Attention. Scaled Dot-Product Attention takes queries, keys and values as input, which are all packed together into matrices. It then uses a special function to compute the output, which helps us understand the relationships between the different inputs. Multi-Head Attention is a type of attention that projects the queries, keys and values multiple times with different projections and then performs the attention function in parallel. This helps us get more accurate results from our input data. tokens in the sequence. To this end, we add “positional encodings” to the input embeddings at the bottoms\n",
            "of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the\n",
            "embeddings, so that the two can be summed. There are many choices of positional encodings, learned\n",
            "and fixed [30].\n",
            "\n",
            "Multi-head attention is a way for a computer model to learn from different parts of a sentence or information at the same time. It uses 8 different heads which help A self-attention layer is like a set of mirrors that can connect any two positions in a sequence. It works quickly and can help learn long-range dependencies. Convolutional layers are also good for learning long-range dependencies but take longer to do so. Recurrent layers are the slowest. Summary: Scientists wanted to create a model to connect all pairs of input and output positions. To do this, they used self-attention layers and point-wise feed-forward layers. They trained the model using a standard dataset and regularization techniques. The model took 12 hours to train on 8 GPUs. A second-grade student can understand that a new model called the Transformer was able to get better scores than previous models when translating from English to German and English to French. It also used less training time than the other models. \n",
            "The Transformer is a machine learning model that can be used to do things like translate English into German or help with English constituency parsing. It comes in many variations and its performance can be improved by changing the values of certain variables like the size of the model (dmodel), the number of layers (h) and the dropout rate (Pdrop). Increasing these values usually results in better performance, and adding dropout can also help avoid over-fitting. \n",
            "This paper talks about a type of machine learning called the Transformer which can help computers understand and translate language. It can be trained faster than other types of models. It was tested on different languages and outperformed all previous models, except for one. This research will help computers to understand and translate language better in the future. Four people - Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio - studied how computers can learn to understand language. They wrote a paper in 2014 to show what they found out. 27 people wrote papers about using machines to understand and summarize language. They used different methods to help computers learn how to read and write better. Many governments in America have passed laws since 2009 to make it harder to register or vote. \n",
            "The law will never be perfect, but it should be applied justly. That is what we are missing in our society today. \n",
            "The Law will never be perfect, but its application should be just. We are missing this in our world, according to some people's opinions.\n"
          ]
        }
      ],
      "source": [
        "summary_text= ' '.join(summary_list)\n",
        "print(summary_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nScXhI01gDiK",
        "outputId": "a8972e26-0030-46b1-da36-7628281df8a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Scientists have developed a new type of machine learning model, called the Transformer. It can help computers understand and translate language better, and it can do this faster than other models. Experiments have shown that the Transformer is more accurate and requires less time to train than other models. This research will help us develop better computer systems for understanding and translating language in the future.\n"
          ]
        }
      ],
      "source": [
        "prompt= \"Summarize this for a second-grade student: \"+summary_text \n",
        "response = openai.Completion.create(\n",
        "model=\"text-davinci-003\",\n",
        "prompt=prompt,\n",
        "temperature=0.7,\n",
        "max_tokens=400,\n",
        "top_p=0.9,\n",
        "frequency_penalty=0.0,\n",
        "presence_penalty=1\n",
        ")\n",
        "print(response[\"choices\"][0][\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_M1V45vgA7N"
      },
      "source": [
        "# -----------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO61cejsZioP"
      },
      "source": [
        "## CardioVascular Paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# -----------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "6c6rKFDDY40V"
      },
      "outputs": [],
      "source": [
        "doc_cardio = fitz.open('/Users/trongphan/Downloads/CardioVascular.pdf') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "1Ghu-iwXY6HV"
      },
      "outputs": [],
      "source": [
        "summary_list_cardio =[]\n",
        "for page in doc_cardio:\n",
        "  text = page.get_text(\"text\")\n",
        "  #print(text)\n",
        "  prompt= text + \"\\n Tl;dr:\"\n",
        "  response = openai.Completion.create(\n",
        "  model=\"text-davinci-003\",\n",
        "  prompt=prompt,\n",
        "  temperature=0.7,\n",
        "  max_tokens=120,\n",
        "  top_p=0.9,\n",
        "  frequency_penalty=0.0,\n",
        "  presence_penalty=1\n",
        "  )\n",
        "  summary_list_cardio.append(response[\"choices\"][0][\"text\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "L0sOJwlHaLzE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " This article reviews the evidence for heritability of cardiovascular disease, as well as what is known about the genetic factors that are associated with it. It highlights how understanding the mechanisms of monogenic cardiovascular disorders has helped us to better understand the pathophysiology of more complex forms of the disease. Additionally, it provides 91 references and 285 citing articles related to the topic.  Monogenic diseases, such as familial hypercholesterolemia, can cause elevated levels of LDL cholesterol and lead to coronary artery disease. These diseases are caused by a deficit in LDL receptors, mutations in the APOB-100 gene, or loss-of-function mutations in genes encoding ATP-binding cassette (ABC) transporters. These conditions can result in increased absorption of cholesterol and LDL synthesis, leading to an increase in plasma LDL levels.  Hypercholesterolemia is caused by high-fat diets, genetic mutations, and other poorly understood causes. Statin therapy helps regulate cholesterol levels. Hypertension is a common disease with a prevalence above 20%, and it can lead to stroke, heart attack, and kidney failure. Monogenic diseases such as molecular mechanisms that mediate salt reabsorption in the kidney can cause hypertension.  Abnormalities in the activity of aldosterone synthase, mutations that alter renal ion channels and transporters, as well as mutations in genes that regulate pathways can cause hypertension or hypotension. These findings have lead to new targets for antihypertensive therapy, including the epithelial sodium channel, other ion channels, and the WNK kinases. Thrombosis and hemostasis also play an important role in cardiovascular disease. \n",
            " Factor V Leiden is a common variant in the factor V gene that can cause an increased risk of thrombosis. Hypertrophic cardiomyopathy is a monogenic cardiac disorder caused by mutations in the genes encoding proteins of the myocardial-contractile apparatus, and its symptoms are highly variable. Figure 3 illustrates the different mutations in cardiac sarcomeric proteins that can cause hypertrophic cardiomyopathy.  Cardiovascular Disease is caused by genetic mutations that affect the head and head–rod junction of the heavy chain, which can lead to pathologic changes early in life and produce severe hypertension. These mutations vary in severity, with some leading to sudden death and heart failure and others causing less severe clinical disease. Additionally, other factors such as gene modifiers, the environment, sex, and acquired conditions can affect the pathologic features and clinical course of Hypertrophic Cardiomyopathy. Furthermore, cardiac arrhythmias are responsible for 450,000 deaths in the US yearly and have been linked to genetic variants  Polymorphism-association studies are used to compare the prevalence of genetic markers in people with certain cardiovascular diseases to that of a control population. This can help narrow down candidate intervals identified by linkage analysis and identify genetic variants underlying complex cardiovascular traits. These studies should be interpreted with caution, as single-nucleotide polymorphisms may be functionally important or just a marker for another disease-causing sequence variant.  Three studies have highlighted the importance of cardiovascular genotyping to establish a molecular diagnosis, to stratify patients according to risk, and especially to guide therapy. The first study examined the prevalence of 112 polymorphisms in 71 candidate genes and found associations with myocardial infarction in both men and women. The second study investigated 62 candidate genes in patients and their siblings with premature myocardial infarction and identified variants in thrombospondin-4, thrombospondin-2, and thrombospondin-1 that showed a statistical association with premature coronary \n",
            "Genome-wide approaches and gene-expression profiling have provided data showing that genetic polymorphisms of proteins involved in drug metabolism, transporters, and targets can have important effects on the efficacy of cardiovascular drugs. Additionally, microarray analyses have been used to define a role for proliferative and inflammatory genes in the development of restenosis after the placement of coronary-artery stents. These findings are important considerations for molecular and clinical diagnosis. \n",
            " The diagnosis of monogenic cardiovascular disorders is typically made through physical examination and routine testing. Research initiatives are focusing on the natural history of these disorders in order to identify high-risk patients, asymptomatic carriers, and nonaffected family members. Current research is also looking into complex traits in more common cardiovascular diseases, which may lead to a molecular diagnosis and influence patient outcomes.  This article discusses the causes and effects of cardiovascular disease, including cholesterol quartet, familial hypercholesterolemia, dietary cholesterol accumulation, disorders of biogenesis and secretion of lipoproteins, mutations in adjacent ABC transporters, a gene important in the regulation of dietary cholesterol absorption, autosomal recessive hypercholesterolemia caused by mutations in a putative LDL receptor adaptor protein, major outcomes in high-risk hypertensive patients, molecular mechanisms of human hypertension, human hypertension caused by mutations in WNK kinases, and a chimaeric 11 beta-hydroxylase/ \n",
            "\n",
            "This article discusses cardiovascular disease and its causes. It mentions hereditary hypertension caused by chimaeric gene duplications, congenitally defective aldosterone biosynthesis, Liddle's syndrome caused by mutations in the beta subunit of the epithelial sodium channel, Gitelman's variant of Bartter's syndrome caused by mutations in the thiazide-sensitive Na-Cl cotransporter, and resistance to activated protein C as a basis for venous thrombosis.  Cardiovascular disease is a serious condition caused by genetic mutations, such as resistance to activated protein C due to an Arg→Gln mutation in the gene for factor V. Other genetic mutations that increase the risk of cardiovascular disease include those related to thrombosis, homozygous siblings with resistance to activated protein C, factor V Leiden (activated protein C resistance), and the G20210A prothrombin gene mutation. Hypertrophic cardiomyopathy is also a common cause of cardiovascular disease. \n",
            "Mutations in sarcomere protein genes, the beta cardiac myosin heavy chain gene, and the cardiac troponin I gene are all causes of dilated cardiomyopathy and hypertrophic cardiomyopathy. The angiotensin-I converting enzyme genotype also influences left ventricular hypertrophy in patients with hypertrophic cardiomyopathy, and its influence varies depending on the disease gene mutation. \n",
            "\n",
            "Studies have found that polymorphisms in the renin-angiotensin-aldosterone system, mutations in cardiac troponin T, and mutations in SCN5A are associated with cardiovascular diseases such as hypertrophic cardiomyopathy, long QT syndrome, and idiopathic ventricular fibrillation. \n",
            "This article discusses the link between cardiovascular diseases and various mutations in sodium channels, potassium channels, and genes. It also discusses high-throughput screening techniques that are used to detect genetic associations with cardiac arrhythmias.  Cardiovascular disease can be linked to various single nucleotide polymorphisms in coding regions of human genes, as well as polymorphisms in thrombospondin, β- and α-adrenergic receptors, and the angiotensin-converting enzyme gene. These polymorphisms are associated with an increased risk of myocardial infarction, congestive heart failure, and a variety of drug responses. The MDR1 genotype is also related to the pharmacokinetics of digoxin.  The insertion/deletion polymorphism of the angiotensin-converting enzyme (ACE) gene has been found to play a role in the response to cardiovascular treatments, such as ACE inhibitors, β-blockers and statins. Studies have shown that this polymorphism can modulate the reversal of left ventricular hypertrophy, endothelial dysfunction, IgA nephropathy, chronic proteinuric nephropathies, and coronary atherosclerosis. \n",
            "This article discusses the effects of common polymorphisms of the beta-adrenergic receptor on agonist-mediated vascular desensitization. It also examines how these polymorphisms can predict in vivo responsiveness and determine prognosis in survivors of myocardial infarction, as well as affect plasma lipid response to atorvastatin and progression of coronary atherosclerosis. Additionally, it looks at a common variant of the cholesteryl ester transfer protein gene and its role in the progression of coronary atherosclerosis, and a common polymorphism associated with antibiotic-induced cardiac arrhyth  This article discusses the diagnosis and treatment of cardiovascular diseases such as hypertension, anticoagulation/thromboembolism, lipids, and hypertrophic cardiomyopathy. It also mentions laboratory molecular diagnosis, which is important for detecting genetically transmitted cardiovascular diseases.  Cardiovascular disease is a major health issue. It includes arrhythmias, pacemakers, defibrillators, coronary disease, myocardial infarction, cardiology, cardiomyopathy, myocarditis, pericarditis, and genetics. Recent research on the topic includes a study on drofolate reductase deficiency and a study on high-dose chemotherapy with H.  Cardiovascular Disease (CVD) is a serious medical condition that can lead to heart attack, stroke, and death. It is caused by a variety of risk factors, including smoking, high blood pressure, high cholesterol, physical inactivity, and diabetes. Prevention is key and includes lifestyle modifications such as quitting smoking, exercising regularly, and maintaining a healthy diet. Early detection and treatment of CVD can help reduce the risk of developing more serious complications.\n"
          ]
        }
      ],
      "source": [
        "summary_text_cardio=' '.join(summary_list_cardio)\n",
        "print(summary_text_cardio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbxHoTB0afoY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
